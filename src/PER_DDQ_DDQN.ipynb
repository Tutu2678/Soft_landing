{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acdaad5-ff58-40aa-a705-6330b622d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from datetime import datetime\n",
    "import pandas as pd  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68cbeaf-32fd-4565-9b94-f84bb5b17fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        \n",
    "#         print('state:', state.shape)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7046d2-05b9-4b28-a347-21c089c42763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, buffer_size, batch_size, priority=False):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer (chosen as multiple of num agents)\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.states = torch.zeros((buffer_size,)+(state_size,)).to(device)\n",
    "        self.next_states = torch.zeros((buffer_size,)+(state_size,)).to(device)\n",
    "        self.actions = torch.zeros(buffer_size,1, dtype=torch.long).to(device)\n",
    "        self.rewards = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n",
    "        self.dones = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n",
    "        self.e = np.zeros((buffer_size, 1), dtype=float)\n",
    "        \n",
    "        self.priority = priority\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.n = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        self.states[self.ptr] = torch.from_numpy(state).to(device)\n",
    "        self.next_states[self.ptr] = torch.from_numpy(next_state).to(device)\n",
    "        \n",
    "        self.actions[self.ptr] = torch.from_numpy(np.asarray(action)).to(device)\n",
    "        self.rewards[self.ptr] = torch.from_numpy(np.asarray(reward)).to(device)\n",
    "        self.dones[self.ptr] = done\n",
    "#         self.actions[self.ptr] = action\n",
    "#         self.rewards[self.ptr] = reward\n",
    "#         self.dones[self.ptr] = done\n",
    "        \n",
    "        self.ptr += 1\n",
    "        if self.ptr >= self.buffer_size:\n",
    "            self.ptr = 0\n",
    "            self.n = self.buffer_size\n",
    "\n",
    "    def sample(self, get_all=False):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        n = len(self)\n",
    "        if get_all:\n",
    "            return self.states[:n], self.actions[:n], self.rewards[:n], self.next_states[:n], self.dones[:n]\n",
    "        # else:\n",
    "        if self.priority:\n",
    "            idx = np.random.choice(n, self.batch_size, replace=False, p=self.e)\n",
    "        else:\n",
    "            idx = np.random.choice(n, self.batch_size, replace=False)\n",
    "        \n",
    "        states = self.states[idx]\n",
    "        next_states = self.next_states[idx]\n",
    "        actions = self.actions[idx]\n",
    "        rewards = self.rewards[idx]\n",
    "        dones = self.dones[idx]\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), idx\n",
    "      \n",
    "    def update_error(self, e, idx=None):\n",
    "        e = torch.abs(e.detach())\n",
    "        e = e / e.sum()\n",
    "        if idx is not None:\n",
    "            self.e[idx] = e.cpu().numpy()\n",
    "        else:\n",
    "            self.e[:len(self)] = e.cpu().numpy()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.n == 0:\n",
    "            return self.ptr\n",
    "        else:\n",
    "            return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6017732-dee6-4ad0-8b02-f07ff9771ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384b501e-ee05-45ed-be87-866b7204cb66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DDQNAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=42, model_type='DQN'):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(state_size, (action_size,), BUFFER_SIZE, BATCH_SIZE)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences, idx = self.memory.sample()\n",
    "                e = self.learn(experiences)\n",
    "                self.memory.update_error(e, idx)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "          \n",
    "    def update_error(self):\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(get_all=True)\n",
    "        with torch.no_grad():\n",
    "            if self.model_type:\n",
    "                old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "                actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, actions)\n",
    "                target = rewards+GAMMA*maxQ*(1-dones)\n",
    "            else: # Normal DQN\n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "                target = rewards+GAMMA*maxQ*(1-dones)\n",
    "                old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            e = old_val - target\n",
    "            self.memory.update_error(e)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## compute and minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.model_type:\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            with torch.no_grad():\n",
    "                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n",
    "                target = rewards+GAMMA*maxQ*(1-dones)\n",
    "        else: # Normal DQN\n",
    "            with torch.no_grad():\n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "                target = rewards+GAMMA*maxQ*(1-dones)\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)   \n",
    "        \n",
    "        loss = F.mse_loss(old_val, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) \n",
    "        \n",
    "        return old_val - target\n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f28474-ec33-4c24-abe4-b58d8707176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs='LunarLander-v2'\n",
    "ls=['DQN','DDQN','Priority']\n",
    "n_episodes=2000\n",
    "max_t=1000\n",
    "eps_start=1.0\n",
    "eps_end=0.01\n",
    "eps_decay=0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484eb151-696a-45d2-80b0-075e2375e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▏                                                               | 344/2000 [17:34<3:00:09,  6.53s/it]"
     ]
    }
   ],
   "source": [
    "res=[]\n",
    "for j in ls:\n",
    "    rewards = []\n",
    "    aver_reward = []\n",
    "    aver = deque(maxlen=100)\n",
    "    \n",
    "    env = gym.make(envs)\n",
    "    env.seed(0)\n",
    "    \n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size=env.action_space.n\n",
    "    agent = DDQNAgent(state_size, action_size, 1, model_type='DDQN')\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    \n",
    "    for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        aver.append(score)     \n",
    "        aver_reward.append(np.mean(aver))\n",
    "        rewards.append(score)\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "    \n",
    "    reward = f\"model/{envs}{j}{n_episodes}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    torch.save(agent.qnetwork_local.state_dict(), reward+'.pt')\n",
    "    res.append(aver_reward)\n",
    "\n",
    "fig = plt.figure()   \n",
    "reward = f'plots/{envs}_result{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "df = pd.DataFrame({'DQN': res[0], 'DDQN': res[1], 'Priority': res[2]})\n",
    "df.to_csv(reward+'.csv')\n",
    "\n",
    "plt.plot(df['DQN'], 'r', label='DQN')\n",
    "plt.plot(df['DDQN'], 'orange', label='DDQN')\n",
    "plt.plot(df['Priority'],'b', label='PER')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title('Learning Curve '+envs)\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "fig.savefig(reward+'.png', dpi=100)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36458e-6436-4d71-b8de-ecb8fbe593f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
